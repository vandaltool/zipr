                    ]:
            """
        """
    # 1 as usual, but we are allowed to perform local CPU
    action = commandline.ProcessCommandLine(sys.argv[1:])
        action = "full"
                    action = globals.restart.type
            # action == "inc" was requested, but no full backup is available
        actual_sig = fileobj.fileobj.get_signature()
                           (actual_sig, globals.gpg_profile.sign_key),
        # Add volume information to manifest
    # Adjust the path to your location and version of Eclipse and Pydev.
            # a limitation in the GPG implementation does not allow for
            # Allow an empty passphrase for the key though to allow a non-empty
# along with duplicity; if not, write to the Free Software Foundation,
        # already have a local partial.  The local partial will already be
        and decrypt it with the current passphrase.  We also want to confirm
          and globals.gpg_profile.recipients
         and globals.gpg_profile.sign_key in globals.gpg_profile.recipients
          and not globals.gpg_profile.sign_key):
         and 'PASSPHRASE' in os.environ ):
    # and refuse to run if it is set.
         and 'SIGN_PASSPHRASE' in os.environ ):
        # and we have to start the process all over again until clean.
# any suggestions.
    archive is synchronized to remote storage.
            # ask the user to enter a new passphrase to avoid an infinite loop
        assert action == "inc" or action == "full", action
    assert backup_chain, col_stats.all_backup_chains
    assert backup_type == "full" or backup_type == "inc"
            assert dup_time.curtime != dup_time.prevtime, "time not moving forward at appropriate pace - system clock issues?"
    assert globals.async_concurrency <= 1
        assert globals.fail_on_volume != vol_num, "Forced assertion for testing at volume %d" % vol_num
    assert globals.keep_chains is not None
    assert globals.remove_time is not None
    assert (isinstance(fileobj, dup_temp.FileobjHooked) and
    assert sig_type in ["full-sig", "new-sig"]
    assuming some hash is available.  Also, if globals.sign_key is
    # Assumptions:
    async_waiters = []
        async_waiters.append(io_scheduler.schedule_task(lambda tdp, dest_filename, vol_num: put(tdp, dest_filename, vol_num),
    at_end = 0
            at_end = gpg.GPGWriteFile(tarblock_iter, tdp.name,
            at_end = gpg.GzipWriteFile(tarblock_iter, tdp.name, globals.volsize)
    # authoritative. figure out which are local spurious (should not
    backend.get(filename, tdp)
            backend.put(tdp, dest_filename)
    backup_chain = col_stats.get_backup_chain_at_time(time)
            backup_ropath = path.ROPath(current_path.index)
    backup_setlist = backup_chain.get_sets_at_time(time)
                                          backup_set.volume_name_dict[vol_num],
                             "         backup then restart the backup from the beginning.") %
    backup_type should be "inc" or "full" and only matters here when
        # bad duplicity, bad doggy!
                base, ext = os.path.splitext(fn)
        base, ext = os.path.splitext(fn)
                    base = fn
            base = fn
    # because the actual I/O concurrency on backends is limited to
                # before we hit the next index, to prevent skipping its first
    # be there) and missing (should be there but are not).
        # better, but for now this will ease the pain.
                # block.
        break
            break
                break
                    break
    bytes_written = 0
        bytes_written = dummy_backup(tarblock_iter)
        bytes_written += waiter()
    #bytes_written += write_manifest(mf, backup_type, backend)
        bytes_written = write_multivol("full", tarblock_iter,
        bytes_written = write_multivol("inc", tarblock_iter,
        calculated_hash = gpg.get_hash(hash_pair[0], vol_path)
                            _("Calculated hash: %s") % calculated_hash,
        # Calculate space we need for at least 2 volumes of full or inc
    # check archive synch with remote, fix if needed
    Check consistency and hostname/directory of last manifest
    # check for disk space and available file handles
    Check for sufficient resources:
    # check if we can reuse an already set (signing_)passphrase
                    check_last_manifest(col_stats) # not needed for full backup
        # Checkpoint after each volume so restart has a place to restart.
    check_resources(action)
    Check the hash of vol_path path against data in volume_info
    Check to make sure passphrase is indeed needed, then get
        class Block:
class Restart:
        class SrcIter:
    Class to aid in restart of inc or full backup.
        cleanup(col_stats)
        # close manifest and rename to final
        # close manifest, send to remote, and rename to final
        # close sig file and rename to final
        # close sig file, send to remote, and rename to final
            code_extra = "%s %d %d" % (util.escape(dest_filename), orig_size, size)
    collated = diffdir.collate2iters(restore_get_patched_rop_iter(col_stats),
    # Collect byte count from all asynchronous jobs; also implicitly waits
            col_stats.backend.delete(ext_remote)
                    col_stats = collections.CollectionsStatus(globals.backend,
    col_stats = collections.CollectionsStatus(globals.backend,
        col_stats.matched_chain_pair[1].end_time < globals.remove_time):
        col_stats.set_values(sig_chain_warning=None)
        # complete in this case (seems we got interrupted before we could move
                continue
                    continue
                           "Continuing restart on file %s.") %
                   "Continuing restart on file %s.") %
        Copy data from src_iter to file at fn
    Copy missing files from remote to local as needed to make sure the local
            copy_raw(src_iter, tdp.name)
        Copy remote file fn to local cache.
# Copyright 2002 Ben Escoto <ben@emerose.org>
# Copyright 2007 Kenneth Loafman <kenneth@loafman.com>
                copy_to_local(fn)
        # Create volume
            current_path = path.ROPath(backup_ropath.index)
    cur_vol = [0]
            cur_vol[0] += 1
                         cur_vol[0], num_vols)
            Data block to return from SrcIter
                data = src_iter.next().data
                                 "%d differences found", diff_count) % diff_count))
    decrypt = action not in ["collection-status"]
        # default. But do with sufficient verbosity.
def check_last_manifest(col_stats):
    def checkManifest(self, mf):
def check_resources(action):
def check_sig_chain(col_stats):
    def check_signature():
def cleanup(col_stats):
    def copy_raw(src_iter, filename):
    def copy_to_local(fn):
def dummy_backup(tarblock_iter):
def full_backup(col_stats):
    def get_fileobj_iter(backup_set):
            def get_footer(self):
    def get_indicies(tarblock_iter):
def get_man_fileobj(backup_type):
    def get_metafiles(filelist):
def get_passphrase(n, action, for_signing = False):
            def get_read_size(self):
def get_sig_fileobj(sig_type):
def incremental_backup(sig_chain):
            def __init__(self, data):
            def __init__(self, fileobj):
    def __init__(self, last_backup):
def list_current(col_stats):
def log_startup_parms(verbosity=log.INFO):
def main():
            def next(self):
def print_statistics(stats, bytes_written):
    def put(tdp, dest_filename, vol_num):
def remove_all_but_n_full(col_stats):
    def remove_local(fn):
def remove_old(col_stats):
    def resolve_basename(fn):
def restart_position_iterator(tarblock_iter):
def restore_add_sig_check(fileobj):
def restore_check_hash(volume_info, vol_path):
def restore(col_stats):
def restore_get_enc_fileobj(backend, filename, volume_info):
def restore_get_patched_rop_iter(col_stats):
    def setLastSaved(self, mf):
    def setParms(self, last_backup):
    def set_times_str(setlist):
def sync_archive(decrypt):
    def validate_block(orig_size, dest_filename):
    def validate_encryption_settings(backup_set, manifest):
def verify(col_stats):
def with_tempdir(fn):
def write_multivol(backup_type, tarblock_iter, man_outfp, sig_outfp, backend):
        # delete final versions of partial files because if we have both, it
    Delete the extraneous files in the current backend
                                    "Deleting backup sets at times:",
                                    "Deleting these files from backend:",
        del_name = globals.archive_dir.append(fn).name
        dest_filename = file_naming.get(backup_type, vol_num,
    # determine what action we're performing and process command line
                                 "%d files compared", total_count) % total_count,
    diff_count = 0; total_count = 0
            diff_count += 1
        diffdir.stats.TotalDestinationSizeChange = bytes_written
    Do full backup of directory to backend, using archive_dir
    Do incremental backup of directory to backend, using archive_dir
    # done on the backends to make them support concurrency.
    # Don't move this lower.  In order to get an exit
        don't need to decrypt the existing volumes on the backend.  To ensure
    # duplicity crashes when PYTHONOPTIMIZE is set, so check
# duplicity -- Encrypted bandwidth efficient backup
# Duplicity is distributed in the hope that it will be useful, but
# Duplicity is free software; you can redistribute it and/or modify it
          duplicity partial versions of the above
            dup_time.setcurtime()
        dup_time.setcurtime()
    dup_time.setcurtime()
        dup_time.setcurtime(globals.current_time)
                        dup_time.setcurtime(globals.restart.end_time)
                        dup_time.setcurtime(globals.restart.time)
                        dup_time.setprevtime(globals.restart.start_time)
        dup_time.setprevtime(sig_chain.end_time)
                           e.__class__.__name__)
                       e.__class__.__name__)
    elif action == "cleanup":
    elif action == "collection-status":
    elif (action == "full"
    elif (action == "inc"
    elif action in ["collection-status",
    elif action == "list-current":
    elif action == "remove-all-but-n-full" or action == "remove-all-inc-of-but-n-full":
    elif action == "remove-old":
    elif action == "sync":
    elif action == "verify":
            elif for_signing:
            elif mf_len - self.start_vol > 0:
                    else:
                else :
                else:
            else:
        else:
    else:
        else:  # attempt incremental
    # enabling concurrency above 1, before adequate work has been
                                          encrypted=globals.encryption)
                                        encrypted=globals.encryption,
    #   - encrypt-key has no passphrase
    Encrypt volumes of tarblock_iter and write to backend
            end_block -= 1
            end_block = start_block
        end_index, end_block = tarblock_iter.get_previous_index()
            end_index = start_index
    # end remote debugger startup
      - enough max open files
    except duplicity.errors.BackendException, e:
    except duplicity.errors.UserError, e:
                except Exception:
        except Exception:
        except Exception, e:
    except Exception, e:
    except gpg.GPGError, e:
    except KeyboardInterrupt, e:
    except KeyError:
        except resource.error:
            except StopIteration:
    except StopIteration:
    except SystemExit, e:
    Execute function and guarantee cleanup of tempdir is called
        exit_val = 1
exit_val = None
    ext_local, ext_remote = col_stats.get_extraneous(globals.extra_clean)
    extraneous = ext_local + ext_remote
    Fake writing to backend, but do go through all the source paths.
    fh = dup_temp.get_fileobj_duppath(globals.archive_dir,
        file.close()
                            filename,
    fileobj.addhook(check_signature)
            fileobj.close()
        fileobj = globals.backend.get_fileobj_read(fn)
    fileobj_iters = map(get_fileobj_iter, backup_setlist)
            fileobj = restore_get_enc_fileobj(globals.backend, vol1_filename,
    fileobj = tdp.filtered_open_with_delete("rb")
        file = open(filename, "wb")
        Files excluded are:
        Files of interest are:
    filestr = "\n".join(extraneous)
            file.write(data)
    finally:
    # Finally, ask the user for the passphrase
    # finally finish the process
        # First check disk space in temp area.
    # First try the environment
        fn()
    # for a full backup, we don't need a password if
    # for an inc backup, we don't need a password if
        # For backend errors, don't show an ugly stack trace by
    for backup_ropath, current_path in collated:
        # force a cleanup operation to get rid of unnecessary old cruft
            for fn in ext_local:
        for fn in filelist:
            for fn in local_missing:
            for fn in local_spurious:
        # For gpg errors, don't show an ugly stack trace by
    for key in local_keys:
    for key in remote_keys:
    for path in path_iter:
        for set in setlist:
    for s in backup_setlist:
        # for testing purposes only - assert on inc or full
    # for them all to complete.
        # For user errors, don't show an ugly stack trace by
                for vol in range(self.start_vol + 1, mf_len + 1):
        for vol_num in volumes:
    for waiter in async_waiters:
                                    "Found old backup sets at the following times:",
                                     "Found the following files to delete:",
# Free Software Foundation; either version 2 of the License, or (at your
                     (freespace, needspace))
                           (freespace, needspace), log.ErrorCode.not_enough_freespace)
        freespace = stats[statvfs.F_FRSIZE] * stats[statvfs.F_BAVAIL]
from duplicity import asyncscheduler
from duplicity import collections
from duplicity import commandline
from duplicity import diffdir
from duplicity import dup_temp
from duplicity import dup_time
from duplicity import file_naming
from duplicity import globals
from duplicity import gpg
from duplicity import log
from duplicity import manifest
from duplicity import patchdir
from duplicity import path
from duplicity import robust
from duplicity import tempdir
from duplicity import util
        from encrypted to non in the middle of a backup chain), so we check
                full_backup(col_stats)
            full_backup(col_stats)
    # full/inc only needs a passphrase for symmetric keys
# General Public License for more details.
    # get current collection status
        """Get file object iterator from backup_set contain given index"""
    Get last signature chain for inc backup, or None if none available
    # get local metafile list
    # get remote metafile list
gettext.install('duplicity')
                gettext.ngettext("%d difference found",
               (gettext.ngettext("%d file compared",
        # get the passphrase if we need to based on action/options
    global exit_val
                    globals.archive_dir.append(fn).delete()
                                                              globals.archive_dir).set_values()
                                              globals.archive_dir).set_values()
                                       globals.backend)
    globals.backend.close()
        globals.extra_clean=True
                                      globals.gpg_profile, globals.volsize)
                        globals.gpg_profile.passphrase = get_passphrase(1, action)
        globals.gpg_profile.passphrase = get_passphrase(1, action)
                    globals.gpg_profile.passphrase = get_passphrase(1, "sync")
            globals.gpg_profile.passphrase = get_passphrase(2, action)
                globals.gpg_profile.passphrase != globals.gpg_profile.signing_passphrase):
            globals.gpg_profile.signing_passphrase = get_passphrase(1, action, True)
    globals.remove_time = col_stats.get_nth_last_full_backup_time(globals.keep_chains)
        globals.restart.checkManifest(mf)
                    globals.restart.last_block))
                    globals.restart = Restart(last_backup)
        globals.restart.setLastSaved(mf)
                   (globals.restart.start_vol,
                           % (globals.restore_dir,),
                                     globals.select)
    # goes here, if needed.
    #   - gpg-agent supplies all, no user interaction
            gpg.GzipWriteFile(src_iter, tdp.name, size=sys.maxint)
                                        gzipped=False,
                                          gzipped=globals.compression)
                                        gzipped=globals.compression)
                                        gzipped=True)
    hash_pair = volume_info.get_best_hash()
                   "However, it will not be deleted.  To remove all your backups, "
                    if action == "full":
        if action == "full":
                if action in ["full", "inc"]:
        if action in ["full", "inc", "cleanup"]:
    if action in ["full", "inc", "restore"]:
    if action == "restore":
        if actual_sig != globals.gpg_profile.sign_key:
                # If both the previous index and this index are done, exit now
        if calculated_hash != hash_pair[1]:
                    if col_stats.all_backup_chains:
    if (col_stats.matched_chain_pair and
                if decrypt:
    if diff_count >= 1:
        if dup_time.curtime == dup_time.prevtime:
        if end_block:
        if end_index is None:
    if exit_val is not None:
# If exit_val is not None, exit with given value at end.
                if ext not in suffixes:
        if ext not in suffixes:
        if "Forced assertion for testing" in str(e):
                if for_signing:
        if for_signing:
    if ( for_signing
        if freespace < needspace:
    if globals.current_time:
            if globals.dry_run:
    if globals.dry_run:
        if globals.encryption:
    if globals.force:
            if (globals.gpg_profile.signing_passphrase and
        if globals.gpg_profile.sign_key:
        if globals.incremental:
    if globals.print_statistics:
    If globals.print_statistics, print stats after adding bytes_written
    if globals.pydevd:
                if globals.remove_all_inc_of_but_n_full_mode and (set.type != "inc") :
        if globals.restore_dir:
    if globals.restore_dir:
        if globals.skip_volume != vol_num: # for testing purposes only
                    if hasattr(self.fileobj, 'name'):
    if hash_pair:
        if key not in local_keys and key not in local_partials:
        if key not in remote_keys or key in local_partials:
            if last_backup.partial:
        if last_backup.time:
                if last_block and tarblock_iter.previous_block > last_block:
    if last_full_time > 0:
            if local_missing:
            if local_missing and (rem_needpass or loc_needpass):
            if local_spurious:
        if maxopen < 1024:
        if (mf_len != self.start_vol) or not (mf_len and self.start_vol):
            if n == 1:
    If n=3, a password is requested and verified. If n=2, the current
if __name__ == "__main__":
    if not action in ["full", "inc"] or not globals.gpg_profile.recipients:
        if not backup_ropath:
        if not backup_ropath.compare_verbose(current_path):
    if not col_stats.all_backup_chains:
    if not col_stats.matched_chain_pair:
        if not current_path:
    if not extraneous:
    if ( not for_signing
        if not globals.dry_run:
    if not globals.encryption or globals.use_agent:
        if not globals.gpg_profile.recipients:
                if not globals.restart:
    if not globals.restart:
    if not globals.restart and action == "inc" and last_full_time < globals.full_force_time:
                if not last_block and not tarblock_iter.previous_block:
            if not last_full_chain:
    if not local_missing and not local_spurious:
            if not pass1 and not globals.gpg_profile.recipients and not for_signing:
            if not pass1 == pass2:
    if not patchdir.Write_ROPaths(globals.local_path,
            if not pr:
                if not res.data:
    if not setlist:
            if not sig_chain:
    if not verified:
    ## if one encryption key is also the signing key assume that the passphrase is identical
    if os.geteuid() == 0:
if os.path.exists(os.path.join(pwd, "../duplicity")):
    if parseresults.encrypted and globals.gpg_profile.sign_key:
        if path.difftype != "deleted":
            if pr.encrypted:
        if pr.manifest:
                if pr.partial:
            if pr.type in ["full-sig", "new-sig"] or pr.manifest:
    # if python is run setuid, it's only partway set,
    if 'PYTHONOPTIMIZE' in os.environ:
            # if remove_all_inc_of_but_n_full_mode mode, remove only incrementals one and not full
    if req_list:
            if self.start_vol == 0:
    ## if signing key is also an encryption key assume that the passphrase is identical
        if size is None:
        if 'size' not in info:
        if size != orig_size:
        if start_block:
        if start_index is None:
            if (tarblock_iter.previous_index == last_index):
            if tarblock_iter.previous_index > last_index:
        if tdp.stat:
        # if there are no recipients (no --encrypt-key), it must be a
            # if the user made a typo in the first passphrase
                    if use_cache and globals.gpg_profile.passphrase:
                    if use_cache and globals.gpg_profile.signing_passphrase:
            if use_cache and n == 2:
        if vol1_filename != backup_set.volume_name_dict[1]:
        if vol_num == 1:
    If volume_info is set, the hash of the file will be checked,
        # If we have a file locally that is unnecessary, delete it.  Also
        # if we have to clean up the last partial, then col_stats are invalidated
        # If we lost our cache, re-get the remote file.  But don't do it if we
import duplicity.errors
import getpass, gzip, os, sys, time, types
import gettext
        import pydevd #@UnresolvedImport
import traceback, platform, statvfs, resource, re
# Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
        in case of the localbackend) rename the temporary file to the target
                incremental_backup(sig_chain)
        index = ()
        index = tuple(globals.restore_dir.split("/"))
        info = backend.query_info([dest_filename])[dest_filename]
            # inputting different passphrases, this affects symmetric+sign.
    Instance in globals.restart if restart in progress.
        instead of copying.
                     + int(0.30 * globals.volsize))
    # intensive tasks while that single upload is happening. This
                           (_("Invalid data - %s hash mismatch for file:") % hash_pair[0],
        # involve downloading the block.  Easier to just redo one block.
    io_scheduler = asyncscheduler.AsyncScheduler(globals.async_concurrency)
    # is an assert put in place to avoid someone accidentally
            isinstance(fileobj.fileobj, gpg.GPGFile)), fileobj
            Iterate over source and return Block of data.
        # it to its final location).
                    "/".join(globals.restart.last_index),
                           ("/".join(last_index), "/".join(tarblock_iter.previous_index)),
                   ("/".join(last_index), "/".join(tarblock_iter.previous_index)),
                 % (''.join(traceback.format_exception(*sys.exc_info()))))
        # Just spin our wheels
        # key, therefore request a passphrase
                    last_backup.delete()
            last_backup = last_full_chain.get_last()
    last backup.  Normal backup will proceed at the start of the
    last_backup_set.check_manifests()
    last_backup_set = col_stats.all_backup_chains[-1].get_last()
        last_block = globals.restart.last_block
    last_block = globals.restart.last_block
            last_full_chain = col_stats.get_last_backup_chain()
    last_full_time = col_stats.get_last_full_backup_time()
    last_index = globals.restart.last_index
                                    len(extraneous))
                                    len(setlist)) +
                    "list-current",
        list_current(col_stats)
    List the files current in the archive (examining signature only)
    local_keys = local_metafiles.keys()
    local_metafiles, local_partials, loc_needpass = get_metafiles(loclist)
    local_missing = []
            local_missing.append(remote_metafiles[key])
                    local_missing = [] # don't download if we can't decrypt
        local_missing.sort()
    local_spurious = []
            local_spurious.append(local_metafiles[key])
        local_spurious.sort()
    loclist = globals.archive_dir.listdir()
        loc_name = base + suffix
                           log.ErrorCode.enryption_mismatch)
                           log.ErrorCode.exception,
                                   log.ErrorCode.generic)
                           log.ErrorCode.get_freespace_failed)
                           log.ErrorCode.get_ulimit_failed)
                       log.ErrorCode.gpg_failed,
                           log.ErrorCode.inc_without_sigs)
                             log.ErrorCode.maxopen_too_low)
                           log.ErrorCode.mismatched_hash)
                           log.ErrorCode.no_restore_files)
""", log.ErrorCode.pythonoptimize_set)
                           log.ErrorCode.restart_file_not_found)
                   log.ErrorCode.restart_file_not_found)
                           log.ErrorCode.restore_dir_not_found)
                           log.ErrorCode.unsigned_volume)
                       log.ErrorCode.user_error,
                           log.ErrorCode.volume_wrong_size, code_extra)
        log.FatalError(
                    log.FatalError(_("Failed to read %s: %s") %
            log.FatalError(_("Fatal Error: Unable to start incremental backup.  "
            log.FatalError(_("File %s was corrupted during upload.") % dest_filename,
            log.FatalError(_("Max open files of %s is too low, should be >= 1024.\n"
            log.FatalError(_("No files found in archive - nothing restored."),
            log.FatalError(_("Restarting backup, but current encryption "
            log.FatalError("%s" % (''.join(traceback.format_exception(*sys.exc_info()))),
            log.FatalError(_("%s not found in archive, no files restored.")
        log.FatalError("%s\n %s\n %s\n %s\n" %
        log.FatalError("%s: %s" % (e.__class__.__name__, e.args[0]),
            log.FatalError("%s: %s" % (e.__class__.__name__, str(e)),
        log.FatalError("%s: %s" % (e.__class__.__name__, str(e)),
            log.FatalError(_("Temp space has %d available, backup needs approx %d.") %
            log.FatalError(_("Unable to get free space on temp."),
            log.FatalError(_("Unable to get max open files."),
            log.FatalError(_("Volume was signed by key %s, not %s") %
                log.FatalError("When using symmetric encryption, the signing passphrase must equal the encryption passphrase.", log.ErrorCode.user_error)
        # Log human-readable version as well as raw numbers for machine consumers
        log.Info(_("Backend error detail: %s")
        log.Info(_("GPG error detail: %s")
        log.Info(_("INT intercepted...exiting."))
        log.Info(_("PASSPHRASE variable not set, asking user."))
            log_info = "%s %s" % (dup_time.timetostring(path.getmtime()),
            log.Info(_("Temp has %d available, backup will use approx %d.") %
                    log_info, True)
        log.Info(_("User error detail: %s")
                    # log it -- main restart heavy lifting is done in write_multivol
    log.Log('=' * 80, verbosity)
    log.Log("Args: %s" % (' '.join(sys.argv),), verbosity)
    log.Log("duplicity 0.6.18 (February 29, 2012)", verbosity)
    log.Log(' '.join(platform.uname()), verbosity)
    log.Log("%s %s" % (sys.executable or sys.platform, sys.version), verbosity)
            log.Log(user_info, log.INFO, log.InfoCode.file_list,
                    log.Notice(_("Cleaning up previous partial %s backup set, restarting." % action))
        log.Notice(_("Copying %s to local cache.") % fn)
        log.Notice(_("Deleting local %s (not authoritative at backend).") % del_name)
                    log.Notice("Deleting set " + set.type + " " + dup_time.timetopretty(set.get_time()))
        log.Notice(gettext.ngettext("Deleting backup set at time:",
        log.Notice(gettext.ngettext("Deleting this file from backend:",
        log.Notice(gettext.ngettext("Found old backup set at the following time:",
        log.Notice(gettext.ngettext("Found the following file to delete:",
        log.Notice(_("Last full backup date:") + " " + dup_time.timetopretty(last_full_time))
        log.Notice(_("Last full backup date: none"))
        log.Notice(_("Last full backup is too old, forcing full backup"))
                    log.Notice(_("Last %s backup left a partial set, restarting." % action))
        log.Notice(_("Local and Remote metadata are synchronized, no sync needed."))
        log.Notice(_("No old backup sets found, nothing deleted."))
                    log.Notice("Not deleting set " + set.type + " " + dup_time.timetopretty(set.get_time()))
                log.Notice("(Not: dry-run) Deleting set " + set.type + " " + dup_time.timetopretty(set.get_time()))
                log.Notice(_("RESTART: Impossible backup state: manifest has %d vols, remote has %d vols.\n"
        log.Notice("Restarting after volume %s, file %s, block %s" %
                log.Notice(_("RESTART: The first volume failed to upload before termination.\n"
                log.Notice(_("RESTART: Volumes %d to %d failed to upload before termination.\n"
        log.Notice(_("Reuse configured PASSPHRASE as SIGN_PASSPHRASE"))
        log.Notice(_("Reuse configured SIGN_PASSPHRASE as PASSPHRASE"))
            log.Notice(_("Synchronizing remote metadata to local cache..."))
                log.Notice(_("Sync would copy the following from remote to local:")
                log.Notice(_("Sync would remove the following spurious local files:")
    log.Notice(_("Verify complete: %s, %s.") %
    log.PrintCollectionStatus(col_stats)
        log.PrintCollectionStatus(col_stats, True)
    log.Progress(None, diffdir.stats.SourceFileSize)
            log.Progress(_('Processed volume %d of %d') % (cur_vol[0], num_vols),
        log.Progress('Processed volume %d' % vol_num, diffdir.stats.SourceFileSize)
    log Python, duplicity, and system versions
log.setup()
    log.shutdown()
    # log some debugging status info
    log_startup_parms(log.INFO)
        log.Warn(_("Current active backup chain is older than specified time.  "
                log.Warn(_("File %s complete in backup set.\n"
        log.Warn(_("File %s missing in backup set.\n"
        log.Warn(_("No extraneous files found, nothing deleted in cleanup."))
            log.Warn(_("No signatures found, switching to full backup."))
        log.Warn("%s\n%s\n%s" %
            log.Warn(_("Unable to delete %s: %s") % (del_name, str(e)))
        # low value for max open files.  Check for safe number.
        # make sure uid/gid match euid/egid
        # Make sure we have enough resouces to run
        manifest = backup_set.get_manifest()
                            _("Manifest hash: %s") % hash_pair[1]),
          manifest - signature files
                                          manifest=True,
                                        manifest=True,
                                        manifest=True)
                                              manifest.volume_info_dict[1])
                                          manifest.volume_info_dict[vol_num])
        man_outfp.close()
            man_outfp.flush()
        man_outfp = get_man_fileobj("full")
                                       man_outfp, sig_outfp,
        man_outfp.to_final()
            man_outfp.to_partial()
        man_outfp.to_remote()
                   "manually purge the repository."))
        maxopen = min([l for l in (soft, hard) if l > -1])
        # means the write of the final version got interrupted.
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
        metafiles = {}
                    metafiles[base] = fn
        mf.add_volume_info(vi)
                    mf.del_volume_info(vol)
        mf.fh = man_outfp
        mf = globals.restart.last_backup.get_local_manifest()
        mf_len = len(mf.volume_info_dict)
                             (mf_len, self.start_vol))
        mf = manifest.Manifest(fh=man_outfp)
        mf.set_dirinfo()
# -*- Mode:Python; indent-tabs-mode:nil; tab-width:4 -*-
                        name = None
                        name = self.fileobj.name
                                   (name, sys.exc_info()),
        need_passphrase = False
                need_passphrase = True
        needspace = (((globals.async_concurrency + 1) * globals.volsize)
        new_man_outfp.close()
        new_man_outfp = get_man_fileobj("inc")
                                       new_man_outfp, new_sig_outfp,
        new_man_outfp.to_final()
        new_man_outfp.to_remote()
                                                  new_sig_outfp)
        new_sig_outfp.close()
        new_sig_outfp = get_sig_fileobj("new-sig")
        new_sig_outfp.to_final()
        new_sig_outfp.to_remote()
    # Next, verify we need to ask the user
    next volume in the set.
                   + "\n" + filestr)
                   + "\n" + filestr + "\n"
                           + "\n" + "\n".join(local_missing))
                           + "\n" + "\n".join(local_spurious))
          non-duplicity files
    # no passphrase if --no-encryption or --use-agent
        # normal backup start
        # note: in the long run backing out changeset 616 might be
        # Note that until after the first volume, all files are temporary.
        # No traceback, just get out
                   "\n" + set_times_str(setlist))
                   "\n" + set_times_str(setlist) + "\n" +
    num_vols = 0
        num_vols += len(s)
    # of concurrency at the backend level. Concurrency 1 is fine
    of the files written to backend.  Also writes manifest file.
    # OK, now we have a stable collection
                             "Old signatures not found and incremental specified"),
                    # only ask for a passphrase if there was a previous backup
                # Only check block number if last_block is also a number
# option) any later version.
        os.close(tempfile)
                os.execve(sys.argv[0], sys.argv[1:], os.environ)
                os.execve(sys.argv[0], sys.argv, os.environ)
        os.setgid(os.getegid())
        os.setuid(os.geteuid())
    os.umask(077)
    @param action: action in progress
    @param action: action to perform
    @param backend: I/O backend for selected protocol
    @param backup_type: type of backup to perform, either 'inc' or 'full'
    @param col_stats: collection status
    @param fn: function to execute
    @param for_signing: true if the passphrase is for a signing key, false if not
    @param man_type: either "full" or "new"
    @param n: verification level for a passphrase being requested
    @param sig_type: either "full-sig" or "new-sig"
    @param tarblock_iter: iterator for current tar block
    parseresults = file_naming.parse(filename)
        partials = {}
                    partials[base] = fn
                                        partial=True)
                                      part_man_filename,
    part_man_filename = file_naming.get(backup_type,
                                      part_sig_filename,
    part_sig_filename = file_naming.get(sig_type,
        pass
            pass
                    pass
                        pass1 = getpass.getpass(_("GnuPG passphrase:")+" ")
                        pass1 = getpass.getpass(_("GnuPG passphrase for signing key:")+" ")
                        pass1 = globals.gpg_profile.passphrase
                    pass1 = globals.gpg_profile.passphrase
                        pass1 = globals.gpg_profile.signing_passphrase
                    pass1 = globals.gpg_profile.signing_passphrase
                pass2 = getpass.getpass(_("Retype passphrase for signing key to confirm: "))
                pass2 = getpass.getpass(_("Retype passphrase to confirm: "))
                pass2 = pass1
        passphrase is the same as the one used for the beginning of the backup.
                    # password for the --encrypt-key
    password is verified. If n=1, a password is requested without
                                   path.get_relative_path())
    path_iter = diffdir.get_combined_path_iter(sig_chain.get_fileobjs(time))
    # per bug https://bugs.launchpad.net/duplicity/+bug/931175
                                      perm_man_filename,
    perm_man_filename = file_naming.get(backup_type,
                                      perm_sig_filename,
    perm_sig_filename = file_naming.get(sig_type,
    picking the filenames.  The path_prefix will determine the names
# Please send mail to me or the mailing list if you find bugs or have
        # plus about 30% of one volume for the signature files.
                    # (possibly) reset action
            pr = file_naming.parse(fn)
        pr = file_naming.parse(fn)
                print _("Cannot use empty passphrase with symmetric encryption!  Please try again.")
        print diffdir.stats.get_stats_logstring(_("Backup Statistics"))
                print _("First and second passphrases do not match!  Please try again.")
    print_statistics(diffdir.stats, bytes_written)
        pr, loc_name, rem_name = resolve_basename(fn)
    Put out fatal error if not sufficient to run
        putsize = tdp.getsize()
pwd = os.path.abspath(os.path.dirname(sys.argv[0]))
        pydevd.settrace()
        pysrc = "/opt/Aptana Studio 3/plugins/org.python.pydev.debug_2.2.4.2011111522/pysrc"
PYTHONOPTIMIZE in the environment causes duplicity to fail to
                    raise StopIteration
    # raising the SystemExit exception.  Cleanup code
    """ reached here, verification passed """
recognize its own backups.  Please remove PYTHONOPTIMIZE from
    remlist = globals.backend.list()
    remote_keys = remote_metafiles.keys()
                                      remote_man_filename)
    remote_man_filename = file_naming.get(backup_type,
    remote_metafiles, ignored, rem_needpass = get_metafiles(remlist)
                                      remote_sig_filename)
    remote_sig_filename = file_naming.get(sig_type, encrypted=globals.encryption,
                    "remove-all-but-n-full",
        remove_all_but_n_full(col_stats)
                    "remove-all-inc-of-but-n-full",
    Remove backup files older than globals.remove_time from backend
    Remove backup files older than the last n full backups.
                    # remove last partial backup and get new collection status
                remove_local(fn)
                    "remove-old",
        remove_old(col_stats)
    remove_old(col_stats)
    req_list = col_stats.get_older_than_required(globals.remove_time)
    Require signature when closing fileobj matches sig in gpg_profile
                   _("Rerun command with --force option to actually delete."))
                    res = Block(self.fileobj.read(self.get_read_size()))
                    # reset the time strings
        # restart from last known position
                             "         Restarting backup at volume %d.") %
                             "         Restart is impossible ... duplicity will clean off the last partial\n"
                             "         Restart is impossible...starting backup from beginning."))
        restart_position_iterator(tarblock_iter)
        restore_add_sig_check(fileobj)
    Restore archive in globals.backend to globals.local_path
        restore(col_stats)
                                  restore_get_patched_rop_iter(col_stats)):
        Retrieve file size *before* calling backend.put(), which may (at least
        return
                return ""
        return ""
    return 0
                return 128 * 1024
    Return a fileobj opened for writing, save results as manifest
    Return a fileobj opened for writing, save results as signature
            return # backend didn't know how to query size
    @return: bytes written
    return bytes_written
    return col_stats.matched_chain_pair[0]
    @return: constant 0 (zero)
            return # error querying file
            return False, hash_pair, calculated_hash
    return fh
    return fileobj
    @return: fileobj opened for writing
    Return iterator of patched ROPaths of desired restore data
        @return: list of duplicity metadata files
        Return metafiles of interest from the file list.
        return metafiles, partials, need_passphrase
        return "\n".join(map(lambda s: dup_time.timetopretty(s.get_time()),
        return None
            return os.environ['PASSPHRASE']
        return os.environ['PASSPHRASE']
            return os.environ['SIGN_PASSPHRASE']
        return os.environ['SIGN_PASSPHRASE']
        @return: (parsedresult, local_name, remote_name)
            return pass1
    @return: passphrase
    return patchdir.tarfiles2rop_iter(tarfiles, index)
    Return plaintext fileobj from encrypted filename on backend
        return (pr, loc_name, fn)
        return putsize
                return res
    Returns number of bytes written.
        """Return start_index and end_index of previous volume"""
        return start_index, start_block, end_index, end_block
        """Return string listing times of sets in setlist"""
    return True, hash_pair, calculated_hash
    @return: true (verified) / false (failed)
    @return: void
    @rtype: boolean
    @rtype: fileobj
    @rtype: int
        @rtype: list
    @rtype: string
    @rtype: void
                   + _("Run duplicity again with the --force option to actually delete."))
    Save manifest in globals.archive_dir gzipped.
    Save signatures in globals.archive_dir gzipped.
    Save them on the backend encrypted as needed.
        # see above for rationale.
See https://bugs.launchpad.net/duplicity/+bug/931175
# See http://www.nongnu.org/duplicity for more information.
                self.data = data
            self.end_time = last_backup.end_time
        self.end_time = None
                    self.fileobj.close()
                self.fileobj = fileobj
                self.last_backup.delete()
        self.last_backup = last_backup
        self.last_block = None
        self.last_block = vi.end_block or 0
        self.last_index = None
        self.last_index = vi.end_index
        self.setParms(last_backup)
            self.start_time = last_backup.start_time
        self.start_time = None
                             (self.start_vol + 1, mf_len, self.start_vol + 1))
        self.start_vol = max(len(last_backup) - 1, 0)
        self.start_vol = None
            self.time = last_backup.time
            self.type = "full"
            self.type = "inc"
        self.type = None
    set, a fatal error will be raised if file not signed by sign_key.
                    set.delete()
                             setlist))
    setlist = col_stats.get_older_than(globals.remove_time)
        setlist.reverse() # save oldest for last
                    # set restart parms from last_backup info
    # set the current time strings again now that we have time separator
    # set the current time strings (make it available for command line processing)
                  set_times_str(req_list),
        # Settings are same, let's check passphrase itself if we are encrypted
                             "settings do not match original settings"),
        # set up iterator
            sig_chain = check_sig_chain(col_stats)
    sig_chain = col_stats.get_signature_chain_at_time(time)
                                                  sig_chain.get_fileobjs(),
                                         sig_chain.get_fileobjs())
    #   - sign-key requires passphrase
                                                 sig_outfp)
        sig_outfp.close()
            sig_outfp.flush()
        sig_outfp = get_sig_fileobj("full-sig")
        sig_outfp.to_final()
            sig_outfp.to_partial()
        sig_outfp.to_remote()
          sigtar - signature files
        size = info['size']
            soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)
    # so make sure to run with euid/egid of root
        # Some environments like Cygwin run with an artificially
    # split up the string.
        src_iter = SrcIter(fileobj)
            start_block -= 1
            start_block = None
    Start/end here
            start_index = ()
        start_index, start_block = tarblock_iter.recall_index()
            stats = os.statvfs(tempfs)
    # status out of the system, you have to call the
    Stop when we have processed the last file and block from the
        # strip off the temp dir and file
    suffixes = [".g", ".gpg", ".z", ".gz", ".part"]
        suffix = file_naming.get_suffix(False, not pr.manifest)
            # symmetric key
        # symmetric key. Therefore, confirm the passphrase
    sync_archive(decrypt)
        sync_archive(True)
    Synchronize local archive manifest file and sig chains to remote archives.
        sys.exit(4)
        sys.exit(e)
        sys.exit(exit_val)
    # sys.exit() function.  Python handles this by
        sys.path.append(pysrc)
    sys.path.insert(0, os.path.abspath(os.path.join(pwd, "../.")))
        tarblock_iter = diffdir.DirDelta(globals.select,
        tarblock_iter = diffdir.DirDelta_WriteSig(globals.select,
        tarblock_iter = diffdir.DirFull(globals.select)
        tarblock_iter = diffdir.DirFull_WriteSig(globals.select,
        tarblock_iter.remember_next_index() # keep track of start index
    tarfiles = map(patchdir.TarFile_FromFileobjs, fileobj_iters)
            tdp.delete()
                                                        (tdp, dest_filename, vol_num)))
        tdp = dup_temp.new_tempduppath(file_naming.parse(dest_filename))
        tdp = dup_temp.new_tempduppath(file_naming.parse(loc_name))
    tdp = dup_temp.new_tempduppath(parseresults)
        tdp.move(globals.archive_dir.append(loc_name))
        tdp.setdata()
        tempdir.default().cleanup()
        tempfile, tempname = tempdir.default().mkstemp()
        tempfs = os.path.sep.join(tempname.split(os.path.sep)[:-2])
      - temp space for volume build
        that the vol1 filename on the server matches the settings of this run.
        that we are using the same passphrase, we manually download volume 1
        that we're using the same encryption settings (i.e. we don't switch
the environment and rerun the backup.
    # The following is for starting remote debugging in Eclipse with Pydev.
        # the most recent block.  Actually checking if we did (via hash) would
        # the passphrase for full and inc is used by --sign-key
    the passphrase from environment, from gpg-agent, or user
                 (_("There are backup set(s) at time(s):"),
    # there is no sign_key and there are recipients
    # these commands don't need a password
        # the sign key can have a different passphrase than the encrypt
    # This assertion must be kept until we have solved the problem
# This file is part of duplicity.
        # this here is to print a list of to-be-removed files (--force is off)
                # this is an 'impossible' state, remove last partial and restart
        This is because the local copy of the manifest is unencrypted and we
        """Thunk run when closing volume file"""
    time = globals.restore_time or dup_time.curtime
            time.sleep(2)
        total_count += 1
            # Traceback and that mess
                try:
            try:
        try:
    try:
    @type  action: string
    @type action: string
    @type backend: callable backend object
    @type backup_type: string
    @type col_stats: CollectionStatus object
    @type fn: callable function
    @type  for_signing: boolean
    @type man_type: string
    @type  n: int
    @type sig_type: string
    @type tarblock_iter: tarblock_iter
# under the terms of the GNU General Public License as published by the
    # Unfortunately, ngettext doesn't handle multiple number variables, so we
                # upload of 1st vol failed, clean and restart
                # upload of N vols failed, fix manifest and restart
    # Upload the collection summary.
                use_cache = False
        use_cache = True
            user_info = "%s %s" % (dup_time.timetopretty(path.getmtime()),
                             "Use 'ulimit -n 1024' or higher to correct.\n") % (maxopen,),
#!/usr/bin/python
                                  util.escape(path.get_relative_path()))
            util.ignore_missing(os.unlink, del_name)
        validate_block(putsize, dest_filename)
        validate_encryption_settings(globals.restart.last_backup, mf)
    verification for the time being.
    verified, hash_pair, calculated_hash = restore_check_hash(volume_info, tdp)
        verify(col_stats)
    Verify files, logging differences
    """ verify hash of the remote file """
# Version 0.6.18 released February 29, 2012
        vi = manifest.VolumeInfo()
        vi = mf.volume_info_dict[self.start_vol]
        vi.set_hash("SHA1", gpg.get_hash("SHA1", tdp))
        vi.set_info(vol_num, *get_indicies(tarblock_iter))
        vol1_filename = file_naming.get(backup_type, 1,
        vol_num = 0
        vol_num += 1
        vol_num = globals.restart.start_vol
        volumes = manifest.get_containing_volumes(index)
    # we have the list of metafiles on both sides. remote is always
        # we said we want to remove them! didn't we, huh?
        # We start one volume back in case we weren't able to finish writing
        When restarting a backup, we have no way to verify that the current
                  _("Which can't be deleted because newer sets depend on them.")))
        while 1:
    while not at_end:
        while tarblock_iter.next():
        while True:
    while True:
# WITHOUT ANY WARRANTY; without even the implied warranty of
        with_tempdir(main)
        # write volume
            yield restore_get_enc_fileobj(backup_set.backend,
# You should have received a copy of the GNU General Public License
